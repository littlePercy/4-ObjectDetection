参考CSDN：
https://blog.csdn.net/dzc_go/article/details/119936225
https://blog.csdn.net/remanented/article/details/79564045
https://cloud.tencent.com/developer/article/1680251
https://blog.csdn.net/Cleo_Gao/article/details/115122223
"""
# 伪彩可视化mask
from PIL import Image

# 读取待训练图片00001
image = Image.open(r"D:\pycharmProjects\Mask-RCNN\PennFudanPed\PennFudanPed\PNGImages\FudanPed00006.png")
# image.show()
# 读取对应图片的掩码图片
mask = Image.open(r"D:\pycharmProjects\Mask-RCNN\PennFudanPed\PennFudanPed\PedMasks\FudanPed00006_mask.png")

# 读取的mask默认为“L”模式，需要转换为“P”模式调用调色板函数
mask = mask.convert("P")

# 针对“P”类图片调用调色板函数
# 看来掩码图片存的不是RGB数值，而是类别index
mask.putpalette([
    0, 0, 0, # 0号像素为黑色
    255, 0, 0, # 1号像素为红色
    255, 255, 0, # 2号像素为黄色
    255, 153, 0, # 3号像素为黄色
])
mask.show()
"""
import os
import torch
import torchvision
import numpy as np
import torch.utils.data
from PIL import Image
import cv2

# 定义数据集类
class PennFudanDataset(torch.utils.data.Dataset):
    def __init__(self, root, transforms=None):
        self.root = root  # 数据集的根路径
        self.transforms = transforms  # 数据集的预处理变形参数

        # 路径组合后返回该路径下的排序过的文件名（排序是为了对齐）
        self.imgs = list(sorted(os.listdir(os.path.join(root, "PNGImages"))))  # self.imgs 是一个全部待训练图片文件名的有序列表
        self.masks = list(sorted(os.listdir(os.path.join(root, "PedMasks"))))  # self.masks 是一个全部掩码图片文件名的有序列表

    # 根据idx对应读取待训练图片以及掩码图片
    def __getitem__(self, idx):
        # 根据idx针对img与mask组合路径
        img_path = os.path.join(self.root, "PNGImages", self.imgs[idx])
        mask_path = os.path.join(self.root, "PedMasks", self.masks[idx])

        # 根据路径读取三色图片并转为RGB格式
        img = Image.open(img_path).convert("RGB")

        # 根据路径读取掩码图片默认“L”格式
        mask = Image.open(mask_path)
        # 将mask转为numpy格式，h*w的矩阵,每个元素是一个颜色id
        mask = np.array(mask)

        # 获取mask中的id组成列表，obj_ids=[0,1,2]
        obj_ids = np.unique(mask)

        # 列表中第一个元素代表背景，不属于我们的目标检测范围，obj_ids=[1,2]
        obj_ids = obj_ids[1:]

        # obj_ids[:,None,None]:[[[1]],[[2]]],masks(2,536,559)
        # 为每一种类别序号都生成一个布尔矩阵，标注每个元素是否属于该颜色
        masks = mask == obj_ids[:, None, None]

        # 为每个目标计算边界框，存入boxes
        num_objs = len(obj_ids)  # 目标个数N
        boxes = []  # 边界框四个坐标的列表，维度(N,4)
        for i in range(num_objs):
            pos = np.where(masks[i])  # pos为mask[i]值为True的地方,也就是属于该颜色类别的id组成的列表
            xmin = np.min(pos[1])  # pos[1]为x坐标，x坐标的最小值
            xmax = np.max(pos[1])
            ymin = np.min(pos[0])  # pos[0]为y坐标
            ymax = np.max(pos[0])
            boxes.append([xmin, ymin, xmax, ymax])

        # 将boxes转化为tensor
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        print(boxes)
        # 初始化类别标签
        labels = torch.ones((num_objs,), dtype=torch.int64)  # labels[1,1] (2,)的array
        masks = torch.as_tensor(masks, dtype=torch.uint8)  # 将masks转换为tensor

        # 将图片序号idx转换为tensor
        image_id = torch.tensor([idx])
        # 计算每个边界框的面积
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        # 假设所有实例都不是人群
        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)  # iscrowd[0,0] (2,)的array

        # 生成一个字典
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["masks"] = masks
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        # 变形transform
        if self.transforms is not None:
            img, target = self.transforms(img, target)

        return img, target

    def __len__(self):
        return len(self.imgs)

if __name__ == '__main__':
    # dataset = PennFudanDataset(r'D:\pycharmProjects\Mask-RCNN\PennFudanPed\PennFudanPed\\')
    # print(dataset[0])
    # print(torch.cuda.is_available())
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
    model.eval()
    transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

    # 使用GPU
    train_on_gpu = torch.cuda.is_available()
    if train_on_gpu:
        model.cuda()
    frame = cv2.imread(r"D:\pycharmProjects\yolo\VOC2012\VOC2012\JPEGImages\2007_001724.jpg")
    blob = transform(frame)
    c, h, w = blob.shape
    input_x = blob.view(1, c, h, w)
    output = model(input_x.cuda())[0]
    boxes = output['boxes'].cpu().detach().numpy()
    scores = output['scores'].cpu().detach().numpy()
    labels = output['labels'].cpu().detach().numpy()
    masks = output['masks'].cpu().detach().numpy()
    print(boxes)
    print(scores)
    print(labels)
    #print(masks)
    index = 0
    color_mask = np.zeros((h, w, c), dtype=np.uint8)
    mv = cv2.split(color_mask)
    for x1, y1, x2, y2 in boxes:
        if scores[index] > 0.5:
            cv2.rectangle(frame, (np.int32(x1), np.int32(y1)),(np.int32(x2), np.int32(y2)), (0, 255, 255), 1, 8, 0)
            mask = np.squeeze(masks[index] > 0.5)
            np.random.randint(0, 256)
            mv[2][mask == 1], mv[1][mask == 1], mv[0][mask == 1] = \
            [np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256)]
            # label_id = labels[index]
            # label_txt = coco_names[str(label_id)]
            # cv2.putText(frame, label_txt, (np.int32(x1), np.int32(y1)), cv.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 255), 1)
            index += 1
    color_mask = cv2.merge(mv)
    result = cv2.addWeighted(frame, 0.5, color_mask, 0.5, 0)
    # cv2.namedWindow("intances segmentation demo", 0)
    # cv2.resizeWindow("intances segmentation demo",800, 800)
    cv2.imshow("intances segmentation demo", result)
    # cv2.imshow('imshow', frame)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
