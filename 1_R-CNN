参考：https://blog.csdn.net/m0_37970224/article/details/85238603

总体分为 四个步骤（下文讲逐步骤分析）：
1.候选区域生成: 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）
2.特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）
3.类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类
4.位置精修： 使用回归器精细修正候选框位置

基本思想：
1、对输入图像进行不同窗口大小的滑窗（方式：从左往右、从上到下）
2、每次滑动时对当前窗口执行分类器(分类器是事先训练好的)，如果当前窗口得到较高的分类概率，则认为检测到了物体。
3、对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分
4、最后采用非极大值抑制(Non-Maximum Suppression, NMS)的方法进行筛选
缺点：
显然这种方法简单暴力，穷举所有可能的窗口，当然面临计算复杂度高的问题、效率低下。

Selective Search 主要思想:
1.使用一种过分割手段，将图像分割成小区域 (1k~2k 个)
2.查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置
3.输出所有曾经存在过的区域，所谓候选区域
其中合并规则如下： 优先合并以下四种区域：

颜色（颜色直方图）相近的
纹理（梯度直方图）相近的
合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -> abcd-efgh -> abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h ->abcd-e-f-g-h ->abcdef-gh -> abcdefgh）
合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。
候选区域算法用分割不同区域的办法来识别潜在的物体。在分割的时候，我们要合并那些在某些方面（如颜色、纹理）类似的小区域。相比滑窗法在不同位置和大小的穷举，候选区域算法将像素分配到少数的分割区域中。所以最终候选区域算法产生的数量比滑窗法少的多，从而大大减少运行物体识别算法的次数。同时候选区域算法所选定的范围天然兼顾了不同的大小和长宽比。
候选区域算法比较重要的特征就是要有较高的召回率。我们要通过这种方法保证拥有物体的区域就在候选区域列表里。所以我们不介意有很多区域什么都有，这都没关系，物体检测算法会过滤掉他们，虽然会浪费一点时间。
效果：
opencv实现了选择性搜索算法，可以给出上千个根据有物体的可能性降序排列的候选区域。
下图是画出了前面200-250个候选区域的效果。一般来说。1000~1200个候选区域基本能胜任物体检测的任务了。
从上图可以看出通过Selective Search算法搜索出2000个候选框，且是大小各异的矩形。

但是，CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。
