1.候选区域的选取和rcnn的操作一样，都是通过Selective Search来选取2K张候选区域。
2.通过CNN来提取特征,这时候使用ROI对特征图进行统一化处理，传到fc层得到分类和标注框结果。

fast-Rcnn在rcnn的基础上做出了改进：

1.只在一张图上做卷积计算，比CNN多重复区域块的卷积提取特征快。

2.softmax多分类器可能比svm二分类器快。

这张图里的ROI Pooling layer以前就没彻底搞懂，原论文：

RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. Pooling is applied independently to each feature map channel, as in standard max pooling. The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets [11] in which there is only one pyramid level. We use the pooling sub-window calculation given in [11].

即将h*w的区域划分成H*W块，每一块h/H × w/W 大小，h/H × w/W 的块中去最大像素作为输出，最终得到H*W的特征图，不管Selective Search得到的图大小如何，最终经过roi pooling 均得到H*W的特征图，送入全连接后维度一致。

这样就统一了特征维度，固定了参数个数，而rcnn中为了送入全连接的维度一致，采用的是在卷积前变形成warp固定大小。

这也是许多网络输入尺寸固定的原因吧，比如lenet手写字符是28*28，resnet50是224*224，都是为了不形变图像而又要保证全连接后维度一致利于分类。

##################官方代码解读
https://zhuanlan.zhihu.com/p/145842317
对于 GeneralizedRCNN 类，其中有4个重要的接口：

################transform
# GeneralizedRCNN.forward(...)
for img in images:
    val = img.shape[-2:]
    assert len(val) == 2
    original_image_sizes.append((val[0], val[1]))
transform主要做2件事：
将输入进行标准化（如FasterRCNN是对 [公式] 输入减 image_mean 再除 image_std）
将图像缩放到固定大小（同时也要对应缩放 targets 中标记框 ）
需要说明，由于把缩放后的图像输入网络，那么网络输出的检测框也是在缩放后的图像上的。但是实际中我们需要的是在原始图像的检测框，为了对应起来，所以需要记录变换前original_images_sizes 。
images, targets = self.transform(images, targets)这里解释一下为何要缩放图像。对于 FasterRCNN，从纯理论上来说确实可以支持任意大小的图片。但是实际中，如果输入图像太大（如6000x4000）会直接撑爆内存。考虑到工程问题，缩放是一个比较稳妥的折衷选择。

##################backbone
将 transform 后的图像输入到 backbone 模块提取特征图
# GeneralizedRCNN.forward(...)
features = self.backbone(images.tensors)backbone 一般为 VGG、ResNet、MobileNet 等网络。

##################rpn
# GeneralizedRCNN.forward(...)
proposals, proposal_losses = self.rpn(images, features, targets)

# roi_heads（即 roi_pooling + 分类）
# GeneralizedRCNN.forward(...)
detections, detector_losses = 
        self.roi_heads(features, proposals, images.image_sizes, targets)
最后经 postprocess 模块（进行 NMS，同时将 box 通过 original_images_size映射回原图）
# GeneralizedRCNN.forward(...)
detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)
